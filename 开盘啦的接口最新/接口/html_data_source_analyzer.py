#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HTMLæ–‡ä»¶æ•°æ®æ¥æºåˆ†æå·¥å…·
åˆ†æHTMLæ–‡ä»¶ä¸­çš„APIæ¥å£å’Œæ•°æ®æ¥æº
"""

import re
import json
import logging
import os
from datetime import datetime
from urllib.parse import urlparse

class HTMLDataSourceAnalyzer:
    def __init__(self):
        self.log_dir = "log"
        self.setup_logging()
        self.api_endpoints = []
        self.external_resources = []
        self.data_sources = []
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)
        
        log_filename = os.path.join(self.log_dir, f"html_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def analyze_html_file(self, file_path):
        """åˆ†æHTMLæ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹åˆ†æHTMLæ–‡ä»¶: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # åˆ†æå„ç§æ•°æ®æº
            self.extract_api_endpoints(html_content)
            self.extract_external_resources(html_content)
            self.extract_image_sources(html_content)
            self.extract_javascript_urls(html_content)
            self.analyze_fetch_requests(html_content)
            self.analyze_data_flow(html_content)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = self.generate_report(file_path)
            return report
            
        except Exception as e:
            self.logger.error(f"åˆ†æHTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def extract_api_endpoints(self, html_content):
        """æå–APIç«¯ç‚¹"""
        self.logger.info("æå–APIç«¯ç‚¹")
        
        # åŒ¹é…fetchè¯·æ±‚ä¸­çš„URL
        fetch_patterns = [
            r'fetch\([\'\"](https?://[^\'\"\s]+)[\'\"]\)',
            r'fetch\(`([^`]+)`\)',
            r'await\s+fetch\([\'\"](https?://[^\'\"\s]+)[\'\"]\)',
        ]
        
        for pattern in fetch_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1]
                
                endpoint_info = {
                    'type': 'API_ENDPOINT',
                    'url': match,
                    'method': 'GET',  # é»˜è®¤GETï¼Œå®é™…å¯èƒ½éœ€è¦æ›´ç»†è‡´çš„åˆ†æ
                    'purpose': self.guess_api_purpose(match)
                }
                self.api_endpoints.append(endpoint_info)
                self.logger.info(f"å‘ç°APIç«¯ç‚¹: {match}")
        
        # åŒ¹é…XMLHttpRequestæˆ–å…¶ä»–AJAXè¯·æ±‚
        ajax_patterns = [
            r'xhr\.open\([\'\"](GET|POST)[\'\"]\s*,\s*[\'\"](https?://[^\'\"\s]+)[\'\"]\)',
            r'\.get\([\'\"](https?://[^\'\"\s]+)[\'\"]\)',
            r'\.post\([\'\"](https?://[^\'\"\s]+)[\'\"]\)',
        ]
        
        for pattern in ajax_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    method = match[0] if len(match) > 1 else 'GET'
                    url = match[1] if len(match) > 1 else match[0]
                else:
                    method = 'GET'
                    url = match
                
                endpoint_info = {
                    'type': 'AJAX_REQUEST',
                    'url': url,
                    'method': method.upper(),
                    'purpose': self.guess_api_purpose(url)
                }
                self.api_endpoints.append(endpoint_info)
                self.logger.info(f"å‘ç°AJAXè¯·æ±‚: {method} {url}")
    
    def extract_external_resources(self, html_content):
        """æå–å¤–éƒ¨èµ„æº"""
        self.logger.info("æå–å¤–éƒ¨èµ„æº")
        
        # CDNèµ„æº
        cdn_pattern = r'src=[\'\"](https?://cdn\.[^\'\"\s]+)[\'\"]\)'
        cdn_matches = re.findall(cdn_pattern, html_content, re.IGNORECASE)
        for match in cdn_matches:
            resource_info = {
                'type': 'CDN_RESOURCE',
                'url': match,
                'resource_type': self.guess_resource_type(match)
            }
            self.external_resources.append(resource_info)
            self.logger.info(f"å‘ç°CDNèµ„æº: {match}")
        
        # è„šæœ¬èµ„æº
        script_pattern = r'<script[^>]+src=[\'\"](https?://[^\'\"\s]+)[\'\"]\)'
        script_matches = re.findall(script_pattern, html_content, re.IGNORECASE)
        for match in script_matches:
            resource_info = {
                'type': 'SCRIPT_RESOURCE',
                'url': match,
                'resource_type': 'javascript'
            }
            self.external_resources.append(resource_info)
            self.logger.info(f"å‘ç°è„šæœ¬èµ„æº: {match}")
    
    def extract_image_sources(self, html_content):
        """æå–å›¾ç‰‡æ¥æº"""
        self.logger.info("æå–å›¾ç‰‡æ¥æº")
        
        # å›¾ç‰‡srcæ¨¡å¼
        img_patterns = [
            r'src=[\'\"](https?://[^\'\"\s]+\.(?:jpg|jpeg|png|gif|svg|webp))[\'\"]\)',
            r'<img[^>]+src=[\'\"](https?://[^\'\"\s]+)[\'\"]\)',
            r'image\.sinajs\.cn[^\'\"\s]*',
            r'http://image\.sinajs\.cn/newchart/daily/[^\'\"\s]*\.gif'
        ]
        
        for pattern in img_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                # è¡¥å…¨ä¸å®Œæ•´çš„URL
                if not match.startswith('http'):
                    match = 'http://' + match
                
                image_info = {
                    'type': 'IMAGE_SOURCE',
                    'url': match,
                    'purpose': self.guess_image_purpose(match)
                }
                self.data_sources.append(image_info)
                self.logger.info(f"å‘ç°å›¾ç‰‡æº: {match}")
    
    def extract_javascript_urls(self, html_content):
        """æå–JavaScriptä¸­çš„URL"""
        self.logger.info("æå–JavaScriptä¸­çš„URL")
        
        # åœ¨JavaScriptä»£ç ä¸­æŸ¥æ‰¾URL
        js_url_patterns = [
            r'[\'\"](https?://[^\'\"\s]+\.(?:php|aspx?|jsp|py|rb)(?:\?[^\'\"\s]*)?)[\'\"]\)',
            r'[\'\"](https?://[a-zA-Z0-9.-]+/[^\'\"\s]*)[\'\"]\)',
        ]
        
        for pattern in js_url_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                
                # è¿‡æ»¤æ‰ä¸€äº›ä¸ç›¸å…³çš„URL
                if any(skip in match.lower() for skip in ['font', 'css', 'bootstrap', 'jquery']):
                    continue
                
                url_info = {
                    'type': 'JAVASCRIPT_URL',
                    'url': match,
                    'purpose': self.guess_api_purpose(match)
                }
                self.data_sources.append(url_info)
                self.logger.info(f"å‘ç°JavaScript URL: {match}")
    
    def analyze_fetch_requests(self, html_content):
        """åˆ†æfetchè¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯"""
        self.logger.info("åˆ†æfetchè¯·æ±‚è¯¦æƒ…")
        
        # æŸ¥æ‰¾å®Œæ•´çš„fetchè°ƒç”¨
        fetch_pattern = r'fetch\s*\(\s*[\'\"](https?://[^\'\"\s]+)[\'\"]\s*(?:,\s*\{([^}]+)\})?\s*\)'
        matches = re.findall(fetch_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        for match in matches:
            url = match[0]
            options = match[1] if len(match) > 1 else ''
            
            # åˆ†æè¯·æ±‚é€‰é¡¹
            method = 'GET'
            headers = {}
            
            if options:
                method_match = re.search(r'method\s*:\s*[\'\"](GET|POST|PUT|DELETE)[\'\"]\)', options, re.IGNORECASE)
                if method_match:
                    method = method_match.group(1).upper()
                
                # æŸ¥æ‰¾headers
                headers_match = re.search(r'headers\s*:\s*\{([^}]+)\}', options, re.IGNORECASE)
                if headers_match:
                    headers_content = headers_match.group(1)
                    header_pairs = re.findall(r'[\'\"]([\w-]+)[\'\"]\s*:\s*[\'\"]([\w\s/.-]+)[\'\"]\)', headers_content)
                    for key, value in header_pairs:
                        headers[key] = value
            
            fetch_info = {
                'type': 'FETCH_REQUEST',
                'url': url,
                'method': method,
                'headers': headers,
                'purpose': self.guess_api_purpose(url),
                'domain': urlparse(url).netloc
            }
            self.api_endpoints.append(fetch_info)
            self.logger.info(f"è¯¦ç»†fetchè¯·æ±‚: {method} {url}")
    
    def analyze_data_flow(self, html_content):
        """åˆ†ææ•°æ®æµ"""
        self.logger.info("åˆ†ææ•°æ®æµ")
        
        # æŸ¥æ‰¾æ•°æ®å¤„ç†å‡½æ•°
        data_processing_patterns = [
            r'processApiData\s*\(',
            r'\.json\(\)\s*\.then',
            r'localStorage\.(?:setItem|getItem)',
            r'JSON\.(?:parse|stringify)',
        ]
        
        data_flow_info = []
        for pattern in data_processing_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            if matches:
                data_flow_info.append({
                    'pattern': pattern,
                    'count': len(matches),
                    'type': self.guess_data_processing_type(pattern)
                })
        
        return data_flow_info
    
    def guess_api_purpose(self, url):
        """çŒœæµ‹APIç”¨é€”"""
        url_lower = url.lower()
        
        if 'longhuvip' in url_lower:
            if 'getytfp_bkhx' in url_lower:
                return 'è·å–æ¿å—è¡Œæƒ…æ•°æ®'
            elif 'getdayzhangting' in url_lower:
                return 'è·å–æ¶¨åœæ•°æ®'
            else:
                return 'é¾™è™æ¦œæ•°æ®æ¥å£'
        elif 'szse.cn' in url_lower:
            return 'æ·±äº¤æ‰€äº¤æ˜“æ—¥å†æ•°æ®'
        elif 'kpl.php' in url_lower:
            return 'å†å²æ•°æ®æŸ¥è¯¢æ¥å£'
        elif 'sinajs.cn' in url_lower:
            return 'æ–°æµªè´¢ç»å›¾è¡¨æ•°æ®'
        else:
            return 'æœªçŸ¥APIæ¥å£'
    
    def guess_image_purpose(self, url):
        """çŒœæµ‹å›¾ç‰‡ç”¨é€”"""
        url_lower = url.lower()
        
        if 'sinajs.cn' in url_lower:
            if 'newchart/daily' in url_lower:
                return 'Kçº¿å›¾è¡¨'
            else:
                return 'æ–°æµªè´¢ç»å›¾è¡¨'
        elif any(ext in url_lower for ext in ['.jpg', '.png', '.gif']):
            return 'é™æ€å›¾ç‰‡èµ„æº'
        else:
            return 'æœªçŸ¥å›¾ç‰‡ç”¨é€”'
    
    def guess_resource_type(self, url):
        """çŒœæµ‹èµ„æºç±»å‹"""
        url_lower = url.lower()
        
        if url_lower.endswith('.js'):
            return 'javascript'
        elif url_lower.endswith('.css'):
            return 'stylesheet'
        elif any(ext in url_lower for ext in ['.jpg', '.png', '.gif', '.svg']):
            return 'image'
        else:
            return 'unknown'
    
    def guess_data_processing_type(self, pattern):
        """çŒœæµ‹æ•°æ®å¤„ç†ç±»å‹"""
        if 'localStorage' in pattern:
            return 'æœ¬åœ°å­˜å‚¨æ“ä½œ'
        elif 'JSON' in pattern:
            return 'JSONæ•°æ®å¤„ç†'
        elif 'processApiData' in pattern:
            return 'APIæ•°æ®å¤„ç†'
        elif '.json()' in pattern:
            return 'HTTPå“åº”è§£æ'
        else:
            return 'æœªçŸ¥æ•°æ®å¤„ç†'
    
    def generate_report(self, file_path):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report = {
            'timestamp': timestamp,
            'analysis_time': datetime.now().isoformat(),
            'file_path': file_path,
            'summary': {
                'total_api_endpoints': len(self.api_endpoints),
                'total_external_resources': len(self.external_resources),
                'total_data_sources': len(self.data_sources),
                'unique_domains': len(set(urlparse(item['url']).netloc for item in self.api_endpoints + self.external_resources + self.data_sources if 'url' in item))
            },
            'api_endpoints': self.api_endpoints,
            'external_resources': self.external_resources,
            'data_sources': self.data_sources,
            'domain_analysis': self.analyze_domains(),
            'technology_stack': self.analyze_technology_stack(),
            'data_flow_patterns': self.analyze_data_flow_patterns()
        }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_filename = os.path.join(self.log_dir, f"html_data_source_report_{timestamp}.json")
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_filename}")
        return report
    
    def analyze_domains(self):
        """åˆ†æåŸŸååˆ†å¸ƒ"""
        domains = {}
        
        for item in self.api_endpoints + self.external_resources + self.data_sources:
            if 'url' in item:
                domain = urlparse(item['url']).netloc
                if domain:
                    if domain not in domains:
                        domains[domain] = {
                            'count': 0,
                            'types': set(),
                            'purposes': set()
                        }
                    domains[domain]['count'] += 1
                    domains[domain]['types'].add(item['type'])
                    if 'purpose' in item:
                        domains[domain]['purposes'].add(item['purpose'])
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for domain in domains:
            domains[domain]['types'] = list(domains[domain]['types'])
            domains[domain]['purposes'] = list(domains[domain]['purposes'])
        
        return domains
    
    def analyze_technology_stack(self):
        """åˆ†ææŠ€æœ¯æ ˆ"""
        tech_stack = {
            'frontend_frameworks': [],
            'chart_libraries': [],
            'data_processing': [],
            'storage_methods': []
        }
        
        # åŸºäºå‘ç°çš„èµ„æºåˆ†ææŠ€æœ¯æ ˆ
        for resource in self.external_resources:
            url = resource['url'].lower()
            if 'echarts' in url:
                tech_stack['chart_libraries'].append('ECharts')
            elif 'jquery' in url:
                tech_stack['frontend_frameworks'].append('jQuery')
            elif 'bootstrap' in url:
                tech_stack['frontend_frameworks'].append('Bootstrap')
        
        # åŸºäºæ•°æ®æºåˆ†æ
        for source in self.data_sources:
            if 'localStorage' in str(source):
                tech_stack['storage_methods'].append('LocalStorage')
        
        return tech_stack
    
    def analyze_data_flow_patterns(self):
        """åˆ†ææ•°æ®æµæ¨¡å¼"""
        patterns = {
            'api_to_localstorage': False,
            'real_time_updates': False,
            'data_export_import': False,
            'chart_integration': False
        }
        
        # åŸºäºAPIç«¯ç‚¹å’Œæ•°æ®æºåˆ†ææ•°æ®æµæ¨¡å¼
        api_urls = [item['url'] for item in self.api_endpoints if 'url' in item]
        
        if any('longhuvip.com' in url for url in api_urls):
            patterns['real_time_updates'] = True
        
        if any('sinajs.cn' in source['url'] for source in self.data_sources if 'url' in source):
            patterns['chart_integration'] = True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¼å…¥å¯¼å‡ºåŠŸèƒ½
        all_content = str(self.api_endpoints + self.external_resources + self.data_sources)
        if 'export' in all_content.lower() or 'import' in all_content.lower():
            patterns['data_export_import'] = True
        
        if 'localStorage' in all_content:
            patterns['api_to_localstorage'] = True
        
        return patterns

def main():
    """ä¸»å‡½æ•°"""
    file_path = r"C:\å´QQçš„AIR\BaiduSyncdisk\600 - åŸæ¡Œé¢\7-25-2.html"
    
    analyzer = HTMLDataSourceAnalyzer()
    report = analyzer.analyze_html_file(file_path)
    
    if report:
        print("\n" + "="*80)
        print("HTMLæ•°æ®æ¥æºåˆ†æç»“æœ")
        print("="*80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“„ æ–‡ä»¶ä¿¡æ¯:")
        print(f"   æ–‡ä»¶è·¯å¾„: {report['file_path']}")
        print(f"   åˆ†ææ—¶é—´: {report['analysis_time']}")
        
        # ç»Ÿè®¡æ¦‚è§ˆ
        summary = report['summary']
        print(f"\nğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ:")
        print(f"   APIç«¯ç‚¹æ•°é‡: {summary['total_api_endpoints']}")
        print(f"   å¤–éƒ¨èµ„æºæ•°é‡: {summary['total_external_resources']}")
        print(f"   æ•°æ®æºæ•°é‡: {summary['total_data_sources']}")
        print(f"   æ¶‰åŠåŸŸåæ•°é‡: {summary['unique_domains']}")
        
        # APIç«¯ç‚¹è¯¦æƒ…
        print(f"\nğŸ”— APIç«¯ç‚¹è¯¦æƒ…:")
        for i, endpoint in enumerate(report['api_endpoints'], 1):
            print(f"   {i}. {endpoint['url']}")
            print(f"      æ–¹æ³•: {endpoint.get('method', 'GET')}")
            print(f"      ç”¨é€”: {endpoint.get('purpose', 'æœªçŸ¥')}")
            print(f"      ç±»å‹: {endpoint.get('type', 'API')}")
            if endpoint.get('headers'):
                print(f"      è¯·æ±‚å¤´: {endpoint['headers']}")
            print()
        
        # æ•°æ®æºè¯¦æƒ…
        print(f"\nğŸ“ˆ æ•°æ®æºè¯¦æƒ…:")
        for i, source in enumerate(report['data_sources'], 1):
            print(f"   {i}. {source['url']}")
            print(f"      ç±»å‹: {source.get('type', 'æœªçŸ¥')}")
            print(f"      ç”¨é€”: {source.get('purpose', 'æœªçŸ¥')}")
            print()
        
        # åŸŸååˆ†æ
        print(f"\nğŸŒ åŸŸååˆ†æ:")
        for domain, info in report['domain_analysis'].items():
            print(f"   {domain}:")
            print(f"      ä½¿ç”¨æ¬¡æ•°: {info['count']}")
            print(f"      èµ„æºç±»å‹: {', '.join(info['types'])}")
            print(f"      ä¸»è¦ç”¨é€”: {', '.join(info['purposes'])}")
            print()
        
        # æŠ€æœ¯æ ˆåˆ†æ
        tech_stack = report['technology_stack']
        print(f"\nâš™ï¸ æŠ€æœ¯æ ˆåˆ†æ:")
        if tech_stack['chart_libraries']:
            print(f"   å›¾è¡¨åº“: {', '.join(tech_stack['chart_libraries'])}")
        if tech_stack['frontend_frameworks']:
            print(f"   å‰ç«¯æ¡†æ¶: {', '.join(tech_stack['frontend_frameworks'])}")
        if tech_stack['storage_methods']:
            print(f"   å­˜å‚¨æ–¹å¼: {', '.join(tech_stack['storage_methods'])}")
        
        # æ•°æ®æµæ¨¡å¼
        patterns = report['data_flow_patterns']
        print(f"\nğŸ”„ æ•°æ®æµæ¨¡å¼:")
        print(f"   APIåˆ°æœ¬åœ°å­˜å‚¨: {'âœ…' if patterns['api_to_localstorage'] else 'âŒ'}")
        print(f"   å®æ—¶æ•°æ®æ›´æ–°: {'âœ…' if patterns['real_time_updates'] else 'âŒ'}")
        print(f"   æ•°æ®å¯¼å…¥å¯¼å‡º: {'âœ…' if patterns['data_export_import'] else 'âŒ'}")
        print(f"   å›¾è¡¨é›†æˆ: {'âœ…' if patterns['chart_integration'] else 'âŒ'}")
        
        print("\n" + "="*80)
        print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: log/html_data_source_report_{report['timestamp']}.json")
        
    else:
        print("âŒ åˆ†æå¤±è´¥")

if __name__ == "__main__":
    main()